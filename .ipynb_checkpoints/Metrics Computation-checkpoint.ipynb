{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "## P-value\n",
    "Let us generate two random samples of size 100 using a normal distribution. First has variance 1 and mean 10. Second has variance 10 and mean 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.randn(100) + 10\n",
    "data_2 = 10*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we want to compare if the two samples have the same distribution using the student t-test. We will need to calculate the standard deviation, the t-value and finally the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value is: 2.22044604925e-16\n"
     ]
    }
   ],
   "source": [
    "var1 = data_1.var(ddof=1)\n",
    "var2 = data_2.var(ddof=1)\n",
    "s = np.sqrt((var1 + var2)/2)\n",
    "t = (data_1.mean() - data_2.mean())/(s*np.sqrt(2/100))\n",
    "p = 1 - stats.t.cdf(t, df=2*100 - 2)\n",
    "\n",
    "print(\"p-value is: \" + str(2*p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare the calculations we got using the build-in functions in scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value is: 1.13408356772e-16\n"
     ]
    }
   ],
   "source": [
    "results = stats.ttest_ind(data_1, data_2)\n",
    "\n",
    "print(\"p-value is: \" + str(2*results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the p-value is very small, we can reject the null hypothesis: the two sample datas have different distributions. This makes sense since we know they have different mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Confusion Matrix\n",
    "Suppose you have trained a model for a 2-class supervised learning problem. You now have an array of target (your real y values) and prediction (your model's predicted y values). You can pass the 2 arrays to the below 2 functions to calculate how accurate your model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(target, prediction):\n",
    "    TP = FP = FN = TN = 0\n",
    "    for i in range(len(target)):\n",
    "        if prediction[i] > 0.5 and target[i] == 1:\n",
    "            TP += 1\n",
    "        elif prediction[i] > 0.5 and target[i] == 0:\n",
    "            FP += 1\n",
    "        elif target[i] == 1:\n",
    "            FN += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    return TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(TP, FP, FN, TN, verbose=True):\n",
    "    # Accuracy\n",
    "    accuracy = (TP+TN)/(TP + FP + FN + TN)\n",
    "    # Precision\n",
    "    precision = TP/(TP+FP)\n",
    "    # Recall\n",
    "    recall = TP/(TP+FN)\n",
    "    # F1-Measure\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    if verbose:\n",
    "        print(\"Accuracy: \" + str(accuracy))\n",
    "        print(\"Precision: \" + str(precision))\n",
    "        print(\"Recall: \" + str(recall))\n",
    "        print(\"F1 measure: \" + str(f1))\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run one simple Logistic Regression written by Daniel Godoy [here](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])\n",
    "y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "logr = LogisticRegression(solver='lbfgs')\n",
    "logr.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Precision: 0.8571428571428571\n",
      "Recall: 0.8571428571428571\n",
      "F1 measure: 0.8571428571428571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP, FP, FN, TN = confusion_matrix(y, y_pred)\n",
    "metrics(TP, FP, FN, TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Functions\n",
    "The easies way to use error functions is by calling the metrics class within scikit-learn. Here are some sample code you can use to calculate the logloss error functions. Other error functions such as MSE and MAE are also implemented in Scikit-learn. I would recommend going to the API description to make sure the used used metric formula is the one you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss is : 0.332912987074\n"
     ]
    }
   ],
   "source": [
    "loss = log_loss(y, y_pred)\n",
    "print('Log Loss is : ' + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
